{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almahmudpias/Bangla-Sign-Language-Digit-Recognition-Using-VGG16-/blob/main/digit_recognition_vgg16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-63XOqfwi1WX",
        "outputId": "f1b18581-18a8-487e-c68a-5b19dedd6c59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwLaBp260vh1",
        "outputId": "9ce876b0-0ae7-4563-ba0a-97509f3cedae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if a GPU is available\n",
        "if tf.test.gpu_device_name():\n",
        "    print('GPU is available.')\n",
        "else:\n",
        "    print('GPU is NOT available.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WQuqZSgjVQP",
        "outputId": "345d8165-ebcf-451b-cbf8-e2021e53f8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n",
            "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n",
            "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n",
            "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ],
      "source": [
        "import  cv2,os\n",
        "from PIL import Image\n",
        "from PIL import Image, ImageOps\n",
        "train_data_path=('/content/gdrive/My Drive/Shongket_Digit_final/augmented_train')\n",
        "test_data_path=('/content/gdrive/My Drive/Shongket_Digit_final/test')\n",
        "val_data_path = ('/content/gdrive/My Drive/Shongket_Digit_final/valid')\n",
        "train_categories=os.listdir(train_data_path)\n",
        "train_categories.sort()\n",
        "test_categories=os.listdir(test_data_path)\n",
        "test_categories.sort()\n",
        "val_categories=os.listdir(val_data_path)\n",
        "val_categories.sort()\n",
        "train_labels=[i for i in range(len(train_categories))]\n",
        "\n",
        "train_label_dict=dict(zip(train_categories,train_labels))\n",
        "\n",
        "print(train_label_dict)\n",
        "print(train_categories)\n",
        "print(train_labels)\n",
        "\n",
        "test_labels=[i for i in range(len(test_categories))]\n",
        "\n",
        "test_label_dict=dict(zip(test_categories,test_labels))\n",
        "\n",
        "print(test_label_dict)\n",
        "print(test_categories)\n",
        "print(test_labels)\n",
        "\n",
        "val_labels=[i for i in range(len(val_categories))]\n",
        "\n",
        "val_label_dict=dict(zip(val_categories,val_labels))\n",
        "\n",
        "print(val_label_dict)\n",
        "print(val_categories)\n",
        "print(val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F28Dh5uRjdB0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial import distance\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.svm import SVC\n",
        "import sklearn.metrics as skmetrics\n",
        "import random\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import glob\n",
        "from PIL import ImageTk, Image\n",
        "from PIL import Image, ImageEnhance\n",
        "from PIL import Image, ImageFilter\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4Pq7FqsjrVB"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "img_size = 224  # VGG16 input size\n",
        "train_data = []\n",
        "train_target = []\n",
        "\n",
        "for category in train_categories:\n",
        "    folder_path = os.path.join(train_data_path, category)\n",
        "    img_names = os.listdir(folder_path)\n",
        "\n",
        "    for img_name in img_names:\n",
        "        img_path = os.path.join(folder_path, img_name)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        try:\n",
        "            # Resize the image to the VGG16 input size\n",
        "            resized = cv2.resize(img, (img_size, img_size))\n",
        "\n",
        "            # Preprocess the image for VGG16 (subtract the mean RGB value of the ImageNet dataset)\n",
        "            preprocessed_img = preprocess_input(resized)\n",
        "\n",
        "            train_data.append(preprocessed_img)\n",
        "            train_target.append(train_label_dict[category])\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Exception:', e)\n",
        "            # If any exception is raised, the exception will be printed here, and the code will continue with the next image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qa2huwqIj6tI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "train_target=np.array(train_target)\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "new_train_target = to_categorical(train_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8adcMvPmkV6H"
      },
      "outputs": [],
      "source": [
        "np.save('train_data',train_data)\n",
        "np.save('train_target',new_train_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVVF0Q9vkb8V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "train_data = np.array(train_data)\n",
        "train_target = np.array(train_target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0SBxt66kjNp",
        "outputId": "a77f57c8-96ff-4c79-d13d-679cc1e0d358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3846, 224, 224, 3) (3846,)\n"
          ]
        }
      ],
      "source": [
        "print(train_data.shape, train_target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C0gPcJBkmz1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "train_data=np.load('train_data.npy')\n",
        "train_target=np.load('train_target.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlT8rSGTkpsR",
        "outputId": "d5dc3758-231a-4dbc-b55a-b5abca796ec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(160, 224, 224, 3) (160,)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define the image size for VGG16 (assuming 224x224)\n",
        "img_size = (224, 224)\n",
        "\n",
        "# Initialize lists to store test data and targets\n",
        "test_data = []\n",
        "test_target = []\n",
        "\n",
        "# Assuming you have a list called test_categories\n",
        "for category in test_categories:\n",
        "    folder_path = os.path.join(test_data_path, category)\n",
        "    img_names = os.listdir(folder_path)\n",
        "\n",
        "    for img_name in img_names:\n",
        "        img_path = os.path.join(folder_path, img_name)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        try:\n",
        "            # Resize the image to match VGG16's input size\n",
        "            img = cv2.resize(img, img_size)\n",
        "\n",
        "            # Preprocess the image for VGG16\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # VGG16 expects RGB format\n",
        "            img = tf.keras.applications.vgg16.preprocess_input(img)\n",
        "\n",
        "            # Append the preprocessed image to test_data\n",
        "            test_data.append(img)\n",
        "\n",
        "            # Append the label to test_target\n",
        "            test_target.append(test_label_dict[category])\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Exception:', e)\n",
        "\n",
        "# Convert the lists to NumPy arrays\n",
        "test_data = np.array(test_data)\n",
        "test_target = np.array(test_target)\n",
        "\n",
        "# Print the shape of test data and targets to verify\n",
        "print(test_data.shape, test_target.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uXDZyk8k9yF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "test_target=np.array(test_target)\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "new_test_target = to_categorical(test_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p_yRrQYlaiz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.save('test_data', test_data)\n",
        "np.save('test_target', new_test_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17xKW9jClc7_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GmJVdePlfRL"
      },
      "outputs": [],
      "source": [
        "X_train = train_data\n",
        "y_train = train_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWLWn6XalhA8"
      },
      "outputs": [],
      "source": [
        "X_test = test_data\n",
        "y_test = new_test_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TBLOx4qljHF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPLVbDuBlk0W"
      },
      "source": [
        "**Transfer Learning (VGG 16)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOapQPb-lpSe",
        "outputId": "6503dd28-c28d-41b6-c9b6-a8b646f43c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with batch size: 64\n",
            "Found 3846 images belonging to 10 classes.\n",
            "Found 161 images belonging to 10 classes.\n",
            "Found 160 images belonging to 10 classes.\n",
            "Epoch 1/50\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3600 - accuracy: 0.1066"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r61/61 [==============================] - 102s 1s/step - loss: 2.3600 - accuracy: 0.1066 - val_loss: 2.2749 - val_accuracy: 0.1553 - lr: 1.0000e-04\n",
            "Epoch 2/50\n",
            "61/61 [==============================] - 73s 1s/step - loss: 2.2781 - accuracy: 0.1443 - val_loss: 2.1981 - val_accuracy: 0.3106 - lr: 1.0000e-04\n",
            "Epoch 3/50\n",
            "61/61 [==============================] - 77s 1s/step - loss: 2.2137 - accuracy: 0.1971 - val_loss: 2.1231 - val_accuracy: 0.4037 - lr: 1.0000e-04\n",
            "Epoch 4/50\n",
            "61/61 [==============================] - 79s 1s/step - loss: 2.1320 - accuracy: 0.2603 - val_loss: 2.0059 - val_accuracy: 0.4224 - lr: 1.0000e-04\n",
            "Epoch 5/50\n",
            "61/61 [==============================] - 75s 1s/step - loss: 2.0377 - accuracy: 0.3372 - val_loss: 1.8650 - val_accuracy: 0.4410 - lr: 1.0000e-04\n",
            "Epoch 6/50\n",
            "61/61 [==============================] - 73s 1s/step - loss: 1.9829 - accuracy: 0.3739 - val_loss: 1.8519 - val_accuracy: 0.4534 - lr: 1.0000e-05\n",
            "Epoch 7/50\n",
            "61/61 [==============================] - 77s 1s/step - loss: 1.9705 - accuracy: 0.3814 - val_loss: 1.8354 - val_accuracy: 0.4720 - lr: 1.0000e-05\n",
            "Epoch 8/50\n",
            "61/61 [==============================] - 77s 1s/step - loss: 1.9512 - accuracy: 0.3926 - val_loss: 1.8211 - val_accuracy: 0.4720 - lr: 1.0000e-05\n",
            "Epoch 9/50\n",
            "61/61 [==============================] - 76s 1s/step - loss: 1.9409 - accuracy: 0.4025 - val_loss: 1.8057 - val_accuracy: 0.4783 - lr: 1.0000e-05\n",
            "Epoch 10/50\n",
            "61/61 [==============================] - 74s 1s/step - loss: 1.9373 - accuracy: 0.3976 - val_loss: 1.7933 - val_accuracy: 0.4845 - lr: 1.0000e-05\n",
            "Epoch 11/50\n",
            "61/61 [==============================] - 78s 1s/step - loss: 1.9246 - accuracy: 0.4030 - val_loss: 1.7919 - val_accuracy: 0.4907 - lr: 1.0000e-06\n",
            "Epoch 12/50\n",
            "61/61 [==============================] - 76s 1s/step - loss: 1.9238 - accuracy: 0.3947 - val_loss: 1.7903 - val_accuracy: 0.4907 - lr: 1.0000e-06\n",
            "Epoch 13/50\n",
            "61/61 [==============================] - 75s 1s/step - loss: 1.9284 - accuracy: 0.3950 - val_loss: 1.7890 - val_accuracy: 0.4907 - lr: 1.0000e-06\n",
            "Epoch 14/50\n",
            "61/61 [==============================] - 77s 1s/step - loss: 1.9253 - accuracy: 0.3996 - val_loss: 1.7875 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 15/50\n",
            "61/61 [==============================] - 79s 1s/step - loss: 1.9257 - accuracy: 0.3963 - val_loss: 1.7862 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 16/50\n",
            "61/61 [==============================] - 77s 1s/step - loss: 1.9258 - accuracy: 0.3916 - val_loss: 1.7849 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 17/50\n",
            "61/61 [==============================] - 77s 1s/step - loss: 1.9169 - accuracy: 0.4069 - val_loss: 1.7837 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 18/50\n",
            "61/61 [==============================] - 75s 1s/step - loss: 1.9171 - accuracy: 0.4085 - val_loss: 1.7827 - val_accuracy: 0.4907 - lr: 1.0000e-06\n",
            "Epoch 19/50\n",
            "61/61 [==============================] - 78s 1s/step - loss: 1.9253 - accuracy: 0.4015 - val_loss: 1.7814 - val_accuracy: 0.4907 - lr: 1.0000e-06\n",
            "Epoch 20/50\n",
            "61/61 [==============================] - 80s 1s/step - loss: 1.9224 - accuracy: 0.3942 - val_loss: 1.7801 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 21/50\n",
            "61/61 [==============================] - 76s 1s/step - loss: 1.9165 - accuracy: 0.4041 - val_loss: 1.7788 - val_accuracy: 0.4907 - lr: 1.0000e-06\n",
            "Epoch 22/50\n",
            "61/61 [==============================] - 79s 1s/step - loss: 1.9104 - accuracy: 0.4184 - val_loss: 1.7776 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 23/50\n",
            "61/61 [==============================] - 76s 1s/step - loss: 1.9180 - accuracy: 0.4087 - val_loss: 1.7764 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 24/50\n",
            "61/61 [==============================] - 75s 1s/step - loss: 1.9188 - accuracy: 0.3983 - val_loss: 1.7753 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 25/50\n",
            "61/61 [==============================] - 77s 1s/step - loss: 1.9039 - accuracy: 0.4194 - val_loss: 1.7741 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 26/50\n",
            "61/61 [==============================] - 78s 1s/step - loss: 1.9080 - accuracy: 0.4155 - val_loss: 1.7728 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 27/50\n",
            "61/61 [==============================] - 76s 1s/step - loss: 1.9045 - accuracy: 0.4124 - val_loss: 1.7718 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 28/50\n",
            "61/61 [==============================] - 76s 1s/step - loss: 1.9023 - accuracy: 0.4168 - val_loss: 1.7705 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 29/50\n",
            "61/61 [==============================] - 75s 1s/step - loss: 1.9024 - accuracy: 0.4093 - val_loss: 1.7694 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 30/50\n",
            "61/61 [==============================] - 77s 1s/step - loss: 1.9084 - accuracy: 0.4147 - val_loss: 1.7680 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 31/50\n",
            "61/61 [==============================] - 80s 1s/step - loss: 1.9101 - accuracy: 0.3996 - val_loss: 1.7667 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 32/50\n",
            "61/61 [==============================] - 76s 1s/step - loss: 1.9085 - accuracy: 0.4176 - val_loss: 1.7653 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 33/50\n",
            "61/61 [==============================] - 78s 1s/step - loss: 1.8997 - accuracy: 0.4215 - val_loss: 1.7640 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 34/50\n",
            "61/61 [==============================] - 80s 1s/step - loss: 1.9050 - accuracy: 0.4184 - val_loss: 1.7627 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 35/50\n",
            "61/61 [==============================] - 77s 1s/step - loss: 1.8899 - accuracy: 0.4251 - val_loss: 1.7616 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 36/50\n",
            "61/61 [==============================] - 78s 1s/step - loss: 1.9046 - accuracy: 0.4041 - val_loss: 1.7603 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 37/50\n",
            "61/61 [==============================] - 76s 1s/step - loss: 1.9029 - accuracy: 0.4046 - val_loss: 1.7589 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 38/50\n",
            "61/61 [==============================] - 79s 1s/step - loss: 1.9008 - accuracy: 0.4202 - val_loss: 1.7577 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 39/50\n",
            "61/61 [==============================] - 76s 1s/step - loss: 1.9046 - accuracy: 0.4056 - val_loss: 1.7566 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 40/50\n",
            "61/61 [==============================] - 78s 1s/step - loss: 1.8943 - accuracy: 0.4129 - val_loss: 1.7554 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 41/50\n",
            "61/61 [==============================] - 76s 1s/step - loss: 1.8931 - accuracy: 0.4124 - val_loss: 1.7541 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 42/50\n",
            "61/61 [==============================] - 78s 1s/step - loss: 1.8899 - accuracy: 0.4215 - val_loss: 1.7528 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 43/50\n",
            "61/61 [==============================] - 78s 1s/step - loss: 1.8980 - accuracy: 0.4145 - val_loss: 1.7516 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 44/50\n",
            "61/61 [==============================] - 75s 1s/step - loss: 1.8932 - accuracy: 0.4233 - val_loss: 1.7504 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 45/50\n",
            "61/61 [==============================] - 77s 1s/step - loss: 1.8989 - accuracy: 0.4116 - val_loss: 1.7489 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 46/50\n",
            "61/61 [==============================] - 76s 1s/step - loss: 1.8863 - accuracy: 0.4103 - val_loss: 1.7475 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 47/50\n",
            "61/61 [==============================] - 76s 1s/step - loss: 1.8822 - accuracy: 0.4332 - val_loss: 1.7463 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 48/50\n",
            "61/61 [==============================] - 75s 1s/step - loss: 1.8841 - accuracy: 0.4168 - val_loss: 1.7450 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 49/50\n",
            "61/61 [==============================] - 76s 1s/step - loss: 1.8861 - accuracy: 0.4145 - val_loss: 1.7436 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 50/50\n",
            "61/61 [==============================] - 80s 1s/step - loss: 1.8830 - accuracy: 0.4176 - val_loss: 1.7424 - val_accuracy: 0.4845 - lr: 1.0000e-06\n",
            "Epoch 1/15\n",
            "61 steps - loss: 2.3600 - accuracy: 0.1066 - val_loss: 2.2749 - val_accuracy: 0.1553\n",
            "Epoch 2/15\n",
            "61 steps - loss: 2.2781 - accuracy: 0.1443 - val_loss: 2.1981 - val_accuracy: 0.3106\n",
            "Epoch 3/15\n",
            "61 steps - loss: 2.2137 - accuracy: 0.1971 - val_loss: 2.1231 - val_accuracy: 0.4037\n",
            "Epoch 4/15\n",
            "61 steps - loss: 2.1320 - accuracy: 0.2603 - val_loss: 2.0059 - val_accuracy: 0.4224\n",
            "Epoch 5/15\n",
            "61 steps - loss: 2.0377 - accuracy: 0.3372 - val_loss: 1.8650 - val_accuracy: 0.4410\n",
            "Epoch 6/15\n",
            "61 steps - loss: 1.9829 - accuracy: 0.3739 - val_loss: 1.8519 - val_accuracy: 0.4534\n",
            "Epoch 7/15\n",
            "61 steps - loss: 1.9705 - accuracy: 0.3814 - val_loss: 1.8354 - val_accuracy: 0.4720\n",
            "Epoch 8/15\n",
            "61 steps - loss: 1.9512 - accuracy: 0.3926 - val_loss: 1.8211 - val_accuracy: 0.4720\n",
            "Epoch 9/15\n",
            "61 steps - loss: 1.9409 - accuracy: 0.4025 - val_loss: 1.8057 - val_accuracy: 0.4783\n",
            "Epoch 10/15\n",
            "61 steps - loss: 1.9373 - accuracy: 0.3976 - val_loss: 1.7933 - val_accuracy: 0.4845\n",
            "Epoch 11/15\n",
            "61 steps - loss: 1.9246 - accuracy: 0.4030 - val_loss: 1.7919 - val_accuracy: 0.4907\n",
            "Epoch 12/15\n",
            "61 steps - loss: 1.9238 - accuracy: 0.3947 - val_loss: 1.7903 - val_accuracy: 0.4907\n",
            "Epoch 13/15\n",
            "61 steps - loss: 1.9284 - accuracy: 0.3950 - val_loss: 1.7890 - val_accuracy: 0.4907\n",
            "Epoch 14/15\n",
            "61 steps - loss: 1.9253 - accuracy: 0.3996 - val_loss: 1.7875 - val_accuracy: 0.4845\n",
            "Epoch 15/15\n",
            "61 steps - loss: 1.9257 - accuracy: 0.3963 - val_loss: 1.7862 - val_accuracy: 0.4845\n",
            "Epoch 16/15\n",
            "61 steps - loss: 1.9258 - accuracy: 0.3916 - val_loss: 1.7849 - val_accuracy: 0.4845\n",
            "Epoch 17/15\n",
            "61 steps - loss: 1.9169 - accuracy: 0.4069 - val_loss: 1.7837 - val_accuracy: 0.4845\n",
            "Epoch 18/15\n",
            "61 steps - loss: 1.9171 - accuracy: 0.4085 - val_loss: 1.7827 - val_accuracy: 0.4907\n",
            "Epoch 19/15\n",
            "61 steps - loss: 1.9253 - accuracy: 0.4015 - val_loss: 1.7814 - val_accuracy: 0.4907\n",
            "Epoch 20/15\n",
            "61 steps - loss: 1.9224 - accuracy: 0.3942 - val_loss: 1.7801 - val_accuracy: 0.4845\n",
            "3/3 [==============================] - 6s 3s/step - loss: 1.6819 - accuracy: 0.5562\n",
            "Test accuracy with batch size 64: 0.5562499761581421\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define the image size (assuming 224x224 for VGG16)\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Batch sizes for training\n",
        "batch_sizes = [64]\n",
        "\n",
        "# Loop through different batch sizes\n",
        "for batch_size in batch_sizes:\n",
        "    print(f\"Training with batch size: {batch_size}\")\n",
        "\n",
        "    # Load the VGG16 model with pre-trained weights (excluding the top classification layer)\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(image_size[0], image_size[1], 3))\n",
        "\n",
        "    # Freeze the layers of the base model\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Add your custom layers for classification\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    predictions = Dense(10, activation='softmax')(x)  # Replace NUM_CLASSES with the number of classes in your dataset\n",
        "\n",
        "    # Create the final model\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # Compile the model with a learning rate of 0.0001\n",
        "    optimizer = Adam(learning_rate=0.0001)  # Set the learning rate to 0.0001\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Implement learning rate schedule\n",
        "    initial_learning_rate = 0.0001\n",
        "\n",
        "    def lr_schedule(epoch):\n",
        "        if epoch < 5:\n",
        "            return initial_learning_rate\n",
        "        elif epoch < 10:\n",
        "            return initial_learning_rate / 10\n",
        "        else:\n",
        "            return initial_learning_rate / 100\n",
        "\n",
        "    lr_callback = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "    # Implement early stopping and model checkpointing\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    model_checkpoint = ModelCheckpoint(f\"best_model_batch{batch_size}.h5\", monitor='val_loss', save_best_only=True)\n",
        "\n",
        "    # Data augmentation for training\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1.0 / 255.0,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "    # Preprocess the validation and test data (no augmentation)\n",
        "    val_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
        "    test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
        "\n",
        "    # Load and augment the training data\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_path,\n",
        "        target_size=image_size,\n",
        "        batch_size=batch_size,  # Use the current batch size\n",
        "        class_mode='categorical')\n",
        "\n",
        "    # Load the validation and test data\n",
        "    val_generator = val_datagen.flow_from_directory(\n",
        "        val_data_path,\n",
        "        target_size=image_size,\n",
        "        batch_size=batch_size,  # Use the current batch size\n",
        "        class_mode='categorical')\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_data_path,\n",
        "        target_size=image_size,\n",
        "        batch_size=batch_size,  # Use the current batch size\n",
        "        class_mode='categorical')\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(train_generator,\n",
        "                        steps_per_epoch=len(train_generator),\n",
        "                        epochs=50,  # Adjust the number of epochs\n",
        "                        validation_data=val_generator,\n",
        "                        validation_steps=len(val_generator),\n",
        "                        callbacks=[lr_callback, early_stopping, model_checkpoint])\n",
        "\n",
        "    for epoch, acc, val_loss, val_acc in zip(range(1, 21), history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']):\n",
        "        print(f'Epoch {epoch}/{15}')\n",
        "        print(f'{len(train_generator)} steps - loss: {history.history[\"loss\"][epoch-1]:.4f} - accuracy: {acc:.4f} - val_loss: {val_loss:.4f} - val_accuracy: {val_acc:.4f}')\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(test_generator, steps=len(test_generator))\n",
        "    print(f\"Test accuracy with batch size {batch_size}: {test_accuracy}\")\n",
        "\n",
        "    # Save the model if needed\n",
        "    model.save(\"vgg16_sign_language_detection_batch_64.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import display, Javascript, Image\n",
        "# from google.colab.output import eval_js\n",
        "# from base64 import b64decode\n",
        "# import cv2\n",
        "# import numpy as np\n",
        "# from tensorflow.keras.models import load_model\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Load your trained model\n",
        "# batch_size = 32  # Replace with your actual batch size\n",
        "# model = load_model('vgg16_sign_language_detection_batch_64.h5')\n",
        "\n",
        "# def take_photo_and_predict(model):\n",
        "#   js = Javascript('''\n",
        "#     async function takePhoto() {\n",
        "#       const div = document.createElement('div');\n",
        "#       const capture = document.createElement('button');\n",
        "#       capture.textContent = 'Capture';\n",
        "#       div.appendChild(capture);\n",
        "\n",
        "#       const video = document.createElement('video');\n",
        "#       const stream = await navigator.mediaDevices.getUserMedia({ 'video': true });\n",
        "\n",
        "#       document.body.appendChild(div);\n",
        "#       div.appendChild(video);\n",
        "#       video.srcObject = stream;\n",
        "#       await video.play();\n",
        "\n",
        "#       // Resize the output to fit the video element.\n",
        "#       google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "#       // Wait for Capture to be clicked.\n",
        "#       await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "#       const canvas = document.createElement('canvas');\n",
        "#       canvas.width = video.videoWidth;\n",
        "#       canvas.height = video.videoHeight;\n",
        "#       canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "#       stream.getVideoTracks()[0].stop();\n",
        "#       div.remove();\n",
        "#       return canvas.toDataURL('image/jpeg');\n",
        "#     }\n",
        "#     ''')\n",
        "#   display(js)\n",
        "#   data = eval_js('takePhoto()')\n",
        "#   binary = b64decode(data.split(',')[1])\n",
        "#   # Now convert this to an image and preprocess it\n",
        "#   image = cv2.imdecode(np.frombuffer(binary, np.uint8), -1)\n",
        "#   image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "#   image = cv2.resize(image, (224, 224))\n",
        "#   image = image / 255.0\n",
        "\n",
        "#   # Make a prediction\n",
        "#   prediction = model.predict(np.expand_dims(image, axis=0))\n",
        "#   predicted_label = np.argmax(prediction)\n",
        "\n",
        "#   # Display the image and the predicted label\n",
        "#   plt.figure(figsize=(4, 4))\n",
        "#   plt.imshow(image)\n",
        "#   plt.title(f'Predicted Label: {predicted_label}')\n",
        "#   plt.axis('off')\n",
        "#   plt.show()\n",
        "\n",
        "# # Capture a photo and make a prediction\n",
        "# try:\n",
        "#   take_photo_and_predict(model)\n",
        "# except Exception as err:\n",
        "#   print(str(err))\n"
      ],
      "metadata": {
        "id": "mnSSOQ1WTDRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "1mbq_6S_Y5cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 32  # Define your batch size\n",
        "# save_path = f'/content/drive/MyDrive/models/vgg16_sign_language_detection_batch_{batch_size}.h5'\n",
        "\n",
        "# # Save the model to the specified path\n",
        "# model.save(save_path)\n"
      ],
      "metadata": {
        "id": "kPZIoqCsWSc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KfXt231sXXi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gd3J-FXx2Ke1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}